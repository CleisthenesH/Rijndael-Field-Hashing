% Copyright 2023 Kieran W Harvie. All rights reserved.

\chapter{Appendix}
\section{Ring Theory 101}
A ring is an algebraic structure that acts like matrix arithmetic.
This means you can add, subtract, and multiply matrices as you would expect.
But you can't necessarily divide or change the order or multiplication. 

For some quick examples consider the following matrices:
\[
A = \begin{bmatrix}
	1&1\\
	0&1\\
\end{bmatrix}
,B = \begin{bmatrix}
	1&0\\
	1&1\\
\end{bmatrix}
,C = \begin{bmatrix}
	1&0\\
	0&0\\
\end{bmatrix}
\]
You can add the matrices:
\[
A+B = B+A =
\begin{bmatrix}
	2&1\\
	1&2\\
\end{bmatrix}
\]
Negate the matrices:
\[
-A = \begin{bmatrix}
	-1&-1\\
	0&-1\\
\end{bmatrix}
\]
And multiply matrices:
\[
C(A+B) = CA+CB = 
\begin{bmatrix}
	2&1\\
	0&0\\
\end{bmatrix}
\]
However the order of multiplication matter:
\[
AB = 
\begin{bmatrix}
	1&1\\
	0&1\\
\end{bmatrix}
\begin{bmatrix}
	1&0\\
	1&1\\
\end{bmatrix}
=
\begin{bmatrix}
	1&1\\
	2&1\\
\end{bmatrix}
\]
\[
BA =
\begin{bmatrix}
	1&0\\
	1&1\\
\end{bmatrix}
\begin{bmatrix}
	1&1\\
	0&1\\
\end{bmatrix}
=
\begin{bmatrix}
	2&1\\
	1&0\\
\end{bmatrix}
\]
And not every matrix is guaranteed an inverse (meaning you can't divide by it):
\[
\det C = \begin{vmatrix}
	1&0\\
	0&0\\
\end{vmatrix} = 0
\]

So when talking about fields why do we care about rings?
Firstly all field are rings,
even though division is not guaranteed in rings they don't forbid it,
so understanding rings helps us understand fields.
But secondly,
because rings are more general then fields they can act as stepping stones.
If we want to show something is a field we can show that it's a ring and then use our knowledge of rings to do the last two steps to show it's a field.
\\

This reason is also why Ring Theory 101 is in the appendix, 
most casual readers will be OK assuming or deducing for themselves how division works.
In this case just skip using rings as a stepping stone.

\subsection{Commutative Rings}
As already shown the order of multiplication may matter in rings.
Rings where order doesn't matter,
i.e. $AB = BA$,
are called commutative.
Commutative Rings are algebraic structures that act like integers.
Where you can add, subtract, and multiply how would expect but still can't divide.
\\

Importantly, because fields are commutative and rings all fields are also commutative rings.
\\

Thinking about commutative rings as integers way will help with the coming sections, 
but the term commutative is used all over mathematics in general and abstract algebra in particular and is worth committing to memory.

\subsection{Polynomial Rings}
Given a commutative ring $R$ we define define the ring of polynomials with coefficients in $R$ as $R[X]$\footnote{Also called polynomials {\bf over} $R$.}.
For example consider the commutative ring of integers $\mathbb{Z}$ then:
\[ \{1+2X,X^2,7\} \subset \mathbb{Z}[X]\]
These polynomials can be "formal" meaning we don't necessarily want or need to substitute in a value for $X$.
All we really care about is the relationship between coefficients,
for example that:
\[(a_0+a_1X)(b_0+b_1X) = a_0b_0 + (a_1b_0+a_0b_1)X+a_1b_1X^2\]

This "formal" view also shows why we need $R$ to be commutative,
since if $R$ is not commutative then we will get some annoying non terms when multiplying polynomials,
for example:
\[(aX)(bX) = aXbX \]
Which doesn't necessarily equal $abX^2$ when $R$ isn't commutative.

\subsubsection{Quotient Polynomial Rings}
\label{appx:quotient-polynomial-ring}
We have already implicitly used the concept of a quotient ring when defining the \hyperref[intro:gf5]{ring of numbers less than 5.}
But to say it explicitly we take a ring,
$\mathbb{Z}$,
and a set within the ring, 
multiples of $5$, 
and say two numbers of the original ring are equal if and only if their difference is in the set.
Also written as the elements being equal {\bf congruent} the set.
\\

For example:
The elements $2$ and $7$ of $\mathbb{Z}$ are equal congruent the multiples of $5$,
often shortened "congruent $5$" or just "congruent" if the set is obvious from context.
\\

This process only produces a ring when the set follows certain properties,
called being a "two-sided ideal",
which are outside the scope of this document.
But all the multiples of a given element are often "two-sided ideals".
\\

This idea also works for polynomial rings.
Consider the quotient ring of $\mathbb{Z}[X]$ modulo $X^2-2$,
detonated $\mathbb{Z}[X] / (X^2-2)$.
The elements $X^3$ and $2X$ and congruent since:
\[X^3 = 2X + X(X^2-2)\]
From this example you may have observed two things:
\begin{itemize}
	\item[1] All the elements of $\mathbb{Z}[X] / (X^2 -2)$ are congruent some degree $1$ or lower polynomial.
	\item[2] $X^2$ is congruent to $2$.
\end{itemize}
So every element of $\mathbb{Z}[X] / (X^2 - 2)$ can be thought as equaling:
\[ a+bX \text{ where $a$ and $b$ are integers and } X^2 = 2\]
Meaning we took $\mathbb{Z}$ and adjoined $\sqrt{2}$, the root of $X^2-2$, to it.

And this observation is mostly correct,
and a big reason as to why we care about Polynomial Rings when talking about Fields.
Firstly the only thing the observation missed is that we could have equally adjoined $-\sqrt{2}$ to $\mathbb{Z}$.
This redundancy is related to the symmetries of $X^2-2$,
and is studied in depth in Galois theory,
but means we can only say we adjoined {\bf a} root of $X^2-2$ not which one.

Secondly the ability to adjoin a new element to a field through quotient polynomial rings will be necessary to understand the Rijndael Field.
But we need one more tool for figuring out when the elements of quotient polynomial rings have reciprocals.

\subsection{Bézout's Identity and Domains}
One of the most useful tools in showing a Ring is a Field is Bézout's Identity.
Before stating the more general form, consider Bézout's Identity for integers:
\begin{displayquote}
Let $x$ and $y$ be integers with the greatest common divisor $d$.
Then for all integers $r$ and $s$ there exists $k$ such that:
	\[xr+ys=kd\]
Furthermore,
there exists integers $r'$ and $s'$ such that $k=1$, i.e.:
	\[xr'+ys'=d\]
\end{displayquote}

\subsubsection{Water Pouring Puzzle}
It can be helpful to think in Bézout's Identity in terms of Water Pouring Puzzles.
For those unfamiliar with these puzzles,
you are given jugs of known sizes and are asked to figure out a sequence of has a specific amount of water at the end.
For example, you are given a 2L and 3L jugs and are ask to produce 1L of water.
You could fill the 3L jug then pour into the 2L jug until it's full, then discard the 2L, leaving 1L in the original jug. 
As required.
\\

Bézout's Identity says that the water in the jugs must always be a multiple of the greatest common divisor of the jugs.
For example if you had a 4L and 6L jug then the water in the jugs is always a multiple of 2L.
Why?
Because we only have the capacity to manipulate water in multiples of 2L,
so after all steps we still have multiples of 2L left.
\\

Bézout's Identity goes on to say that that there alway exits a sequence of moves that get us to the greatest common divisor of water.
This isn't as intuitive to see as the previous statement,
but see that you can remove multiples of the smaller jug from the larger till less water is left in the larger jug then the volume of the smaller.
This effectively creates a new smaller jug with which we can repeat this process till the jugs are the same size,
leaving the greatest common reminder.

\subsubsection{Proving Reciprocation}
To see how Bézout's Identity might might be useful in showing that elements of a ring have a reciprocal,
and hence can be divided by,
\hyperref[intro:gf5]{consider the example given in the introduction about integers less then 5.}
Constructing a multiplication table only works because $5$ is a small number,
consider a larger prime like $113$ how do we show that a $18$ has a reciprocal in this set?
Well since $113$ is prime and $18 < 113$ their greatest common divisor is $1$.
From Bézout's Identity there must exist numbers $r$ and $s$ such that:
\[ 113\times r+18\times s = 1\]
Although not strictly necessary for this argument,
sufficient $r$ and $s$ can be calculated\footnote{
	An efficient way to calculate $r$ and $s$ for the general case is called Euclid's Extended Algorithm. 
	While outside the scope, if you understand this argument you are able to understand the algorithm and I suggest looking it up.} 
as $-7$ and $44$,
giving:
\[ 113\times (-7)+18\times 44 = 1\]
This can be rewritten as:
\[18\times 44 = 1+113\times 7\]
Hence $18\times44$ has remainder $1$ when divided by $113$.
Making $44$ the reciprocal of $18$ for the ring of integers less than $113$.
\\

This argument works just as well for all non-zero integers less than $113$ hence all non-zero integers have a reciprocal making this commutative ring a field.
This argument also for any set of integers less than a prime number, hence all such rings are actually fields.
\\

A similar argument shows that the ring of integers less than a given number $n$ are fields {\bf only} work for prime number.
Otherwise there is a number $m$ that's greatest common divisor with $n$, labeled $d$, is greater than $1$.
And through Bézout's Identity we know that for all $r$ and $s$ we have $k$ such that:
\[ nr+ms = kd > d > 1\]
Hence we can't divide by $m$ and the ring isn't a field.

\subsubsection{Application to Polynomial Rings}
Rings where Bézout's Identity apply are called "Bézout Domains".
Domains are algebraic structure in-between rings and fields where $ab=0$ implies $a=0$ or $b=0$ and Bézout Domains are a subtype of domain.
It's outside of scope to detail another type of algebraic structure,
but it is important to say that Polynomial Rings are a type of Bézout Domain.
\\

This means we can prove reciprocation in a Quotient Polynomial Rings in the same way we did with integers.
Consider $\mathbb{Q}[X]/(X^2-2)$ from \hyperref[appx:quotient-polynomial-ring]{earlier discussion} we know this ring is like taking $\mathbb{Q}$ and adjoining $\pm\sqrt{2}$ to it.
But $\mathbb{Q}$ is a field and not a ring!
Surly we should be able to recover division after adjoining $\pm\sqrt{2}$.
\\

Proof:
If $x$ is the root of a degree $1$ or less polynomial in $\mathbb{Q}[X]$ then $x$ is rational since:
\[ ax+b = 0 \rightarrow x = -\frac{b}{a}\]
Hence $X^2-2$ is relatively prime to all elements of $\mathbb{Q}[X]$ of degree $1$ or less since all roots of $X^2-2$ are irrational.
From Bézout's Identity this means that for any degree $1$ or lower element $p$ of $\mathbb{Q}[X]$ we can find elements $r(X)$ and $s(X)$ of $\mathbb{Q}[X]$ such that:
\[(X^2-2)r(X)+p(X)s(X) = 1\]
Meaning every element of $\mathbb{Q}[X]$ of degree $1$ or less can be reciprocated.
And since $X^2$ is degree $2$ every element of $\mathbb{Q}[X]/(X^2-1)$ is congruent to an element of degree $1$ or less.
Hence every element of $\mathbb{Q}[X] / (X^2 - 2)$ is invertible,
making it a field.

\subsection{Conclusion}
\label{appx:ring-conclusion}
Being an appendix the main goal of Ring Theory 101 is to communicate key ideas around rings for the main text.
These ideas are:
\begin{itemize}
	\item Rings are more general fields where order of multiplication can matter but without division.
	\item Rings where order of multiplication doesn't matter are called commuting rings.
	\item Given a commuting ring $R$ we can make a new ring $R[X]$ of polynomials with coefficients in that ring.
	\item Bézout's Identity is a great tool for finding out when an element of a ring has a reciprocal.
	\item Conceptually Quotient Polynomial Rings are like taking the base ring and adjoining a new element to it.
\end{itemize}
\section{Adjoining a Number to a Field}
\label{appx:adjoining}
$F(a)$ means the smallest field including both the field $F$ and the number $a$ and is referred as adjoining $a$ to $F$.
As suggested by \hyperref[appx:ring-conclusion]{Ring Theory 101's conclusion}, a standard way to do this is to consider polynomials with coefficients in $F$, denoted $F[X]$, and find a polynomial $p(x)\in F[X]$ such $p(a)=0$ consider the quotient ring of $F[X]/q(X)$,
using Bézout's identity when necessary.
\\

But it's really important not to confuse polynomials with coefficients in $F$, denoted $F[X]$, polynomials with coefficients in $F$ evaluated at $a$, denoted $F[a]$, and the field $F$ adjoined with $F(a)$.
What makes this issue more confusing is that when $a$ is the root of a polynomial in $F$\footnote{This property is called $a$ being algebraic over $F$} we do have $F[a]=F(a)$.
\\

Without lost of generality let $f$ be the minimal polynomial such that $f(a)=0$.
Let $g$ be a different polynomial in $F[X]$.
By the minimality of $f$ either $g(a)=0$ or $f$ and $g$ are relatively prime.
If $g(a)=0$ then $g=0$ in $F[a]$ meaning it doesn't need a reciprocal,
hence $f$ and $g$ are relatively prime,
and hence from Bézout's identity we have polynomial $r$,$s$ in $F[X]$ such that:
\[f(x)r(x)+g(x)s(x) = 1\]
But when we substitute in $a$ we have:
\[g(a)s(a) = 1\]
Hence $g$ has a reciprocal in $F[a]$ and from this, with a bit more legwork, we have $F[a] = F(a)$
\\

But when $a$ isn't algebraic this isn't true.
Consider a number $a$ that isn't the solution of some polynomial in $F$.
Then $F(a) \neq F[a]$ because $a^{-1} \not \in F$.
For example there is no element $\pi^{-1}\not\in F[\pi]$.
\\

As much as I stress that $F[a]$ doesn't necessarily equal $F(a)$ the fact that for most contexts it does means confusion is likely.
Its the missing apostrophe of field theory,
as much as I warn you about it dont worry about it.
Like missing apostrophise someone will tell you.

\section{Generator Tables}
\label{appx:gen-table}
For completeness sake I've included the index and power tables when $03$ is the generator.
In table look-up, the columns are lowest half byte and the row is the highest.
For example the index of $A3$ is $6F$.
In both case the lookup for $00$ is arbitrary and chosen to simplify some algorithms.

\newpage

\centerline{\bf Index:}
\centerline{
\begin{tabular}{|c|cccc|cccc|cccc|cccc|}
	\hline
	&$\cdot$0&$\cdot$1&$\cdot$2&$\cdot$3&$\cdot$4&$\cdot$5&$\cdot$6&$\cdot$7&$\cdot$8&$\cdot$9&$\cdot$A&$\cdot$B&$\cdot$C&$\cdot$D&$\cdot$E&$\cdot$F\\
	\hline
	0$\cdot$&00&FF&19&01&32&02&1A&C6&4B&C7&1B&68&33&EE&DF&03\\
	1$\cdot$&64&04&E0&0E&34&8D&81&EF&4C&71&08&C8&F8&69&1C&C1\\
	2$\cdot$&7D&C2&1D&B5&F9&B9&27&6A&4D&E4&A6&72&9A&C9&09&78\\
	3$\cdot$&65&2F&8A&05&21&0F&E1&24&12&F0&82&45&35&93&DA&8E\\
	4$\cdot$&96&8F&DB&BD&36&D0&CE&94&13&5C&D2&F1&40&46&83&38\\
	5$\cdot$&66&DD&FD&30&BF&06&8B&62&B3&25&E2&98&22&88&91&10\\
	6$\cdot$&7E&6E&48&C3&A3&B6&1E&42&3A&6B&28&54&FA&85&3D&BA\\
	7$\cdot$&2B&79&0A&15&9B&9F&5E&CA&4E&D4&AC&E5&F3&73&A7&57\\
	8$\cdot$&AF&58&A8&50&F4&EA&D6&74&4F&AE&E9&D5&E7&E6&AD&E8\\
	9$\cdot$&2C&D7&75&7A&EB&16&0B&F5&59&CB&5F&B0&9C&A9&51&A0\\
	A$\cdot$&7F&0C&F6&6F&17&C4&49&EC&D8&43&1F&2D&A4&76&7B&B7\\
	B$\cdot$&CC&BB&3E&5A&FB&60&B1&86&3B&52&A1&6C&AA&55&29&9D\\
	C$\cdot$&97&B2&87&90&61&BE&DC&FC&BC&95&CF&CD&37&3F&5B&D1\\
	D$\cdot$&53&39&84&3C&41&A2&6D&47&14&2A&9E&5D&56&F2&D3&AB\\
	E$\cdot$&44&11&92&D9&23&20&2E&89&B4&7C&B8&26&77&99&E3&A5\\
	F$\cdot$&67&4A&ED&DE&C5&31&FE&18&0D&63&8C&80&C0&F7&70&07\\
	\hline
\end{tabular}
}
\vspace{5mm}

\centerline{\bf Power:}
\centerline{
\begin{tabular}{|c|cccc|cccc|cccc|cccc|}
	\hline
	&$\cdot$0&$\cdot$1&$\cdot$2&$\cdot$3&$\cdot$4&$\cdot$5&$\cdot$6&$\cdot$7&$\cdot$8&$\cdot$9&$\cdot$A&$\cdot$B&$\cdot$C&$\cdot$D&$\cdot$E&$\cdot$F\\
	\hline
	0$\cdot$&01&03&05&0F&11&33&55&FF&1A&2E&72&96&A1&F8&13&35\\
	1$\cdot$&5F&E1&38&48&D8&73&95&A4&F7&02&06&0A&1E&22&66&AA\\
	2$\cdot$&E5&34&5C&E4&37&59&EB&26&6A&BE&D9&70&90&AB&E6&31\\
	3$\cdot$&53&F5&04&0C&14&3C&44&CC&4F&D1&68&B8&D3&6E&B2&CD\\
	4$\cdot$&4C&D4&67&A9&E0&3B&4D&D7&62&A6&F1&08&18&28&78&88\\
	5$\cdot$&83&9E&B9&D0&6B&BD&DC&7F&81&98&B3&CE&49&DB&76&9A\\
	6$\cdot$&B5&C4&57&F9&10&30&50&F0&0B&1D&27&69&BB&D6&61&A3\\
	7$\cdot$&FE&19&2B&7D&87&92&AD&EC&2F&71&93&AE&E9&20&60&A0\\
	8$\cdot$&FB&16&3A&4E&D2&6D&B7&C2&5D&E7&32&56&FA&15&3F&41\\
	9$\cdot$&C3&5E&E2&3D&47&C9&40&C0&5B&ED&2C&74&9C&BF&DA&75\\
	A$\cdot$&9F&BA&D5&64&AC&EF&2A&7E&82&9D&BC&DF&7A&8E&89&80\\
	B$\cdot$&9B&B6&C1&58&E8&23&65&AF&EA&25&6F&B1&C8&43&C5&54\\
	C$\cdot$&FC&1F&21&63&A5&F4&07&09&1B&2D&77&99&B0&CB&46&CA\\
	D$\cdot$&45&CF&4A&DE&79&8B&86&91&A8&E3&3E&42&C6&51&F3&0E\\
	E$\cdot$&12&36&5A&EE&29&7B&8D&8C&8F&8A&85&94&A7&F2&0D&17\\
	F$\cdot$&39&4B&DD&7C&84&97&A2&FD&1C&24&6C&B4&C7&52&F6&01\\
	\hline
\end{tabular}
}

\section{Side-Channel}
\label{appx:side-channel}
I've said in a few places that using generators to simplify field arithmetic makes the implementation susceptible to side-channel attacks.
But how?
And what is a side-channel attack?
\\

A side-channel is something about the implementation of an algorithm or protocol that lets an attacker get information,
rather than something inherently wrong with the algorithm or protocol itself. 
\\

The easiest to explain side-channel is the timing.
Consider a guarded door and to get access you have to tell the guard a password.
You tell the guard two different incorrect passwords but the guard rejects one immediately but waits till you're half way through for the second one.
You might reasonably conclude that first password is wholly wrong but that the first half of the second one is correct.
By doing this enough times and keeping track of the time the guard rejects the password you can probably figure out the password.
You guessing the password has nothing to do with the cryptography of a password but the implementation by the guard,
instead they should have waited till you've said the whole password before rejecting it, 
and hence is a side-channel attack.
\\

Using a look-up table to compute field operations opens you up to a caching side-channel attack.
\\

Because the look-up table is to large to be stored in the CPU registers it will be accessed from memory.
Accessing the table from memory means it's likely to be cashed.
The table being cached means someone with the ability to monitor cache access can see what field elements you're using,
and maybe work backwards to get some of the string key that was hashed.
Hence leaking the string.
\\

This makes using look-up tables unsuitable for application where the string keys are sensitive information.
\\

Before closing this topic I want to bring up that side-channels can include the physical properties of computers as well.
Consider a naïve attempt at stopping a timing attack where the guard waits to reject the password but is still noticeably paying less attention.
An attacker can still guess the timing from when the guard stops paying attention.
Similarly idling a computer till a set amount of time passes for all password validation attempts might cause a change in power consumption or a noticeable reduction in cooling. 
Hence even power-draw or acoustic properties of a computer are possible side-channels for attacks.

\section{Matrix Arithmetic}
\subsection{Diagonalization}
The description of Gaussian Elimination and the claim that it works with any invertible matrix may make some readers that half remember a linear algebra course nervous.
This is because process described is similar to another process, diagonalization, which is not guaranteed to work just because the input matrix is invertible.
\\

A diagonal matrix is one where all non-diagonal elements are zero and transcribed as $\Diag$:
\[\begin{bmatrix}1&0&0&0\\0&3&0&0\\0&0&2&0\\0&0&0&7\end{bmatrix} = \Diag(1,3,2,7)\]
Such matrices are important they are the same as the matrix for the elementary row operation of scaling the rows,
making matrix multiplication invoking them conceptually and computationally easy.
For example:
\[\Diag(a_0,a_1,\dots a_n)\Diag(b_0,b_1,\dots b_n) = \Diag(a_0b_0,a_1b_1,\dots a_nb_n)\]
\\

Diagonalization is the process of taking a matrix $M$ and finding a diagonal matrix $D$ and invertible matrix $P$ such that:
\footnote{Often equivalently written as $M = PDP^{-1}$}
\[MP = PD\]
And this process is important because it enables the simplification of calculations by canceling $P$'s and utilizing diagonal matrix multiplication:
\begin{equation*}
\begin{aligned}
 M^3 =& (PDP^{-1})^3 \\
 =& (PDP^{-1})(PDP^{-1})(PDP^{-1}) \\
 =& PD(P^{-1} P)D(P^{-1} P)DP^{-1} \\
 =& PD^3P^{-1} \\
 =& P\Diag(a_k)^3P^{-1} \\
 =& P\Diag(a_k^3)P^{-1} \\
\end{aligned}
\end{equation*}
\\

But unfortunately diagonalization isn't guaranteed for invertible matrices.
The conditions of when it works involves something called eigenvalues,
which are outside the scope of this document.
But what is concerning is how similar this looks to Gaussian Elimination,
which given $MA =B$ involves finding an invertible $P$ such that:
\[M'A =PMA = PB\]
Where $M'$ is $M$ after some elementary row operations.
What is important to notice is that in diagonalization $P$ is on the right side of $M$ and the left side of $D$.
But during Gaussian Elimination $P$ is on the left side of both $M$ and $B$.
Since matrix multiplication isn't commutative this makes the processes very different and lays nerves to rest. 
\\

Readers interesting in learning more about diagonalization and it's uses are encouraged to first loop up eigenvalues and then "The Jordan Normal Form".

\subsection{Vandermonde Polynomial}
\label{appx:vandermonde-polynomial}

This proof of the determinant of the Vandermonde Matrix,
the Vandermonde Polynomial,
relies on the readers familiarity with three properties of the determinant.

\begin{enumerate}
	\item[1] If a row has only one non-zero element then the determinant is equal to that element times by the matrix with that row and column removed. \\
	\item[2] Multiplying a row by a constant multiplies the determinant by the same constant. \\
	\item[3] Adding a multiple of one row to another doesn't change the determinant. \\
\end{enumerate}

For those familiar with these properties skip the next paragraph,
for those not remember that geometrically the determinant is the signed $n$-dimensional volume of the parallelotope of the matrices column.
You don't need to know what that means rigorously\footnote{And if you aren't familiar with these properties then this description alone probably doesn't help.} but can think of it like getting the volume of a regular cube.
In this case the first property is like how the volume of a cube is its depth times its cross-section,
the second property is like how if you double a single cube's side length you double it's area,
and the third property is like describing the cubes points relative to other points instead the origin. 
\\

The first step of finding the determinate is to subtract $v_0$ times the previous column from each column,
this doesn't change the determinate:
\begin{equation*}
\begin{aligned}
\begin{vmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{vmatrix} =&
\begin{vmatrix} 
	1&x_0-x_0&x_0^2-x_0x_0&\dots&x_0^n-x_0x_0^{n-1} \\
	1&x_1-x_0&x_1^2-x_0x_1&\dots&x_1^n-x_0x_1^{n-1}\\
	1&x_2-x_0&x_2^2-x_0x_2&\dots&x_2^n-x_0x_2^{n-1} \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n-x_0&x_n^2-x_0x_n&\dots&x_n^n-x_0x_n^{n-1} \\
\end{vmatrix} \\
=&
\begin{vmatrix} 
	1&0&0&\dots&0 \\
	1&x_1-x_0&(x_1-x_0)x_1&\dots&(x_1-x_0)x_1^{n-1}\\
	1&x_2-x_0&(x_2-x_0)x_2&\dots&(x_2-x_0)x_2^{n-1} \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n-x_0&(x_n-x_0)x_n&\dots&(x_n-x_0)x_n^{n-1} \\
\end{vmatrix} \\
\end{aligned}
\end{equation*}
Since there is a row with where the only non-zero element is $1$,
so the determinant is the same if we removed that row and column.
\[
\begin{vmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{vmatrix}  
=
\begin{vmatrix} 
	x_1-x_0&(x_1-x_0)x_1&\dots&(x_1-x_0)x_1^{n-1}\\
	x_2-x_0&(x_2-x_0)x_2&\dots&(x_2-x_0)x_2^{n-1} \\
	\vdots&\vdots&\ddots&\vdots \\
	x_n-x_0&(x_n-x_0)x_n&\dots&(x_n-x_0)x_n^{n-1} \\
\end{vmatrix} 
\]
Now observe that we can factor $(x_1-x_0)$ from the top row, 
and $(x_2-x_0)$ from the next,
and so on:
\[
\begin{vmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{vmatrix}  
=\prod_{k=1}^n(x_k-x_0)
\begin{vmatrix} 
	1&x_1&\dots&x_1^{n-1}\\
	1&x_2&\dots&x_2^{n-1} \\
	\vdots&\vdots&\ddots&\vdots \\
	1&x_n&\dots&x_n^{n-1} \\
\end{vmatrix} 
\]
Which is the $n^\text{th}$ dimensional Vandermonde polynomial in term of the $(n-1)^\text{th}$,
hence induction gives the required result.

\section{Hardware and Instruction Sets}
\subsection{CLMUL Instruction Set}
Carry-less multiplication basically polynomial multiplication in GF$(2)$.
\subsection{AES Instruction Set}
