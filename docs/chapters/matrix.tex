% Copyright 2023 Kieran W Harvie. All rights reserved.

\chapter{Matrix Methods}
From an abstract algebra a matrix is actually a type of tensor as field[
it the underlying object is just a ring it's called a module.
Say how walking through a $3\times 3$ examples is useful to show only field operations is used.
set spell not working?

MAKE SURE TO USE THE APPENDIX!!

\section{Gaussian Elimination}
\subsection{(pointlessly) Abstract Algebra Preamble}
From an abstract algebra viewpoint,
the choice of dual space basis is arbitrary provided they have the right span.
We can get such a new dual basis $\mathbf{e'_i}$ from an old basis $\mathbf{e_j}$, provided $T$ is non-singular, through:
\[\mathbf{e'_i} = T(\{\mathbf{e_j}\})\]
So we can transform relationship between the coefficients of tensors in one basis it to another,
and if $T$ is affine then the new coefficients are directly transformed through $T$.
\\

Three particular choices of $T$ are swapping two the basis elements,
multiplying a basis element by a non-zero constant,
and adding a multiple of basis element index to another.

\subsection{(useful) Matrix Preamble}
\label{matrix:useful-preamble}
Abstract tensor analysis is cool,
for the type of people that like that type of thing,
but the useful result of the previous subsection is that for matrices $a$, $b$, and $M$, plus an invertible matrix $T$ we have:
\[ M^j_ia^i = b^i \Leftrightarrow\, T^j_kM^k_ia^i = T^j_kb^k \]
(And this result can be verified directly by multiplying by $T$ or $T^{-1}$ and short-circuiting the previous preamble).
\\

Bellow are examples of the three previous particualr choices of $T$,
hence forth referred to as elementary row operations,
for $3\times 3$ matrices:
\begin{equation*}
\begin{aligned}
\begin{bmatrix}
	1&0&0 \\
	0&0&1 \\
	0&1&0 \\
\end{bmatrix}
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{21} & m_{22} & m_{23} \\
	m_{31} & m_{32} & m_{33} \\
\end{bmatrix}
=&
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{31} & m_{32} & m_{33} \\
	m_{21} & m_{22} & m_{23} \\
\end{bmatrix} \\
\begin{bmatrix}
	1&0&0 \\
	0&a&0 \\
	0&0&1 \\
\end{bmatrix}
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{21} & m_{22} & m_{23} \\
	m_{31} & m_{32} & m_{33} \\
\end{bmatrix}
=&
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	am_{21} & am_{22} & am_{23} \\
	m_{31} & m_{32} & m_{33} \\
\end{bmatrix} \\
\begin{bmatrix}
	1&0&0 \\
	0&1&0 \\
	0&a&1 \\
\end{bmatrix}
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{21} & m_{22} & m_{23} \\
	m_{31} & m_{32} & m_{33} \\
\end{bmatrix}
=&
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{21} & m_{22} & m_{23} \\
	m_{31}+am_{21} & m_{32}+am_{22} & m_{33}+am_{23} \\
\end{bmatrix} \\
\end{aligned}
\end{equation*}

Since these elementary row operations can be represented through matrices their composition can be represented through matrix multiplication.
Hence we can build more complex row operations from multiplying a chain of elementary row operations together.
\\

For example say we want a transform $T$ that takes the a $3\times 3$ matrix and makes it first column $[1,0,0]$.
Assuming $m_{1n} \neq 0$, we have:
\begin{equation*}
\begin{aligned}
\begin{bmatrix}
	m_{11}^{-1}&0&0 \\
	0&m_{22}^{-1}&0 \\
	0&0&m_{33}^{-1} \\
\end{bmatrix}
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{21} & m_{22} & m_{23} \\
	m_{31} & m_{32} & m_{33} \\
\end{bmatrix}
=&
\begin{bmatrix}
	1 & m_{12}m_{11}^{-1} & m_{13}m_{11}^{-1} \\
	1 & m_{22}m_{12}^{-1} & m_{23}m_{12}^{-1} \\
	1 & m_{32} m_{32}^{-1}& m_{33} m_{31}^{-1} \\
\end{bmatrix} \\
\begin{bmatrix}
	1&0&0 \\
	-1&1&0 \\
	-1&0&1 \\
\end{bmatrix}
\begin{bmatrix}
	1 & m_{12}m_{11}^{-1} & m_{13}m_{11}^{-1} \\
	1 & m_{22}m_{12}^{-1} & m_{23}m_{12}^{-1} \\
	1 & m_{32} m_{32}^{-1}& m_{33} m_{31}^{-1} \\
\end{bmatrix} 
=&
\begin{bmatrix}
	1 & m_{12}m_{11}^{-1} & m_{13}m_{11}^{-1} \\
	0 & m_{22}m_{12}^{-1}-m_{12}m_{11}^{-1} & m_{23}m_{12}^{-1} -m_{13}m_{11}^{-1}\\
	0 & m_{32} m_{32}^{-1}-m_{12}m_{11}^{-1}& m_{23} m_{31}^{-1} -m_{13}m_{11}^{-1}\\
\end{bmatrix} \\
\end{aligned}
\end{equation*}
Hence:
\[T =
\begin{bmatrix}
	1&0&0 \\
	-1&1&0 \\
	-1&0&1 \\
\end{bmatrix}
\begin{bmatrix}
	m_{11}^{-1}&0&0 \\
	0&m_{22}^{-1}&0 \\
	0&0&m_{33}^{-1} \\
\end{bmatrix}
\]
\\

Gaussian Elimination is simply using this process to find a matrix $T$ such that $T(M) = I$ by interatvly zeroing the outer, non-diagonal, elments:
\[
\begin{bmatrix}
	m_{11} & m_{12} & m_{13} \\
	m_{21} & m_{22} & m_{23} \\
	m_{31} & m_{32} & m_{33} \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
	1&0&0\\
	0 & m'_{22} & m'_{23} \\
	0 & m'_{32} & m'_{33} \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
	1&0&0\\
	0 & 1 & 0 \\
	0 & 0 & m''_{33} \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
	1&0&0\\
	0 & 1 & 0 \\
	0 & 0 & 1 \\
\end{bmatrix}
\]
This calculates $M^{-1}$ as the product of matrices representing elemetry row operations.
Note that this always works when $M$ is invertable, adafsd 
but we may need to swap row such that errent zeros put directly into their destination to avoid dividing by zero.

Rrwrite to bring in the opening.
Jordan normal form??

\subsection{The Algorithm}
Matrices can easily be represented as 2d-arrays,
elementary row operations aren't hard to implement either,
and the iterative nature of final equation directly suggests the structure of the algorithm.
However there is one more observation 
instead of keeping in memory all the elementary row operation, just directly apply them to the $B$ as we go.
Augmented matrices are excellent at facilitating this.

\section{Application to Hashing}
Consider an array of $n$ strings, $s_i$, and an array of $n$ values, $a_i$.
Let their be some function $f$ that converts a string to $n$ field elements.
Let $m_{i,j}$ be the $j$th field element for string $s_i$.
Construct the matrix $M$ from $m_{i,j}$ and the column vector $A$ from $a_i$ in the unusual ways.
If $M$ is invertible, then we can use the above algorithm obtains a column vector $B$ such that:
\[MB = A\]
Which in element form is:
\[\sum_{j=1}^nb_jf(s_i)_j = a_i\]
Which defines the hash function:
\[h(s)=\sum_{j=1}^nb_jf(s)_j\]
Such that:
\[h(s_i) = a_i\]
As required.

\subsection{Example}
Consider the strings:
\[s_1 = \text{"APPLE"},\quad s_2 = \text{"ORANGE"} \]
The hash values:
\[a_1 = 01,\quad a_2 = 02\]
The expression $f(s_i)_j$ maps the $j$th character in the $i$th string to it's place in the alphabet.
Relevant conversions done bellow,
full conversion can be found in the \hyperref[theory:byte]{theory section}:

\begin{center}
\begin{tabular}{|c|c|c|}
	\hline
	Character & Decimal & Field Element \\	
	\hline
	A &  1 & 01 \\
	O & 15 & 0A \\
	P & 16 & 10 \\
	R & 18 & 12 \\
	\hline
\end{tabular}
\end{center}

Hence the requirement for $B$ is such:
\[ 
\begin{bmatrix}
	01&10\\
	0A&12\\
\end{bmatrix}
\begin{bmatrix}
	b_1 \\ b_2\\
\end{bmatrix}
=
\begin{bmatrix}
	01\\ 02 \\
\end{bmatrix}
\]
Which has solution:
\[
\begin{bmatrix}
	b_1 \\ b_2\\
\end{bmatrix}
=
\begin{bmatrix}
	18\\F8 \\
\end{bmatrix}
\]
And gives the hash function:
\[h(s) = 18\times f(s)_1+F8\times f(s)_2\]

% This isn't how h is defined, likely a hang-over from an old definition.
%
%\subsection{Uniqueness}
%A useful consequence of the invertibility of $M$ is that:
%\begin{equation*}
%\begin{aligned}
%	Ma_0 = Ma_1 \Rightarrow\,& M(a_0-a_1) = 0 \\
%	\Rightarrow\,& M^{-1}M(a_0-a_1) = M^{-1}0 \\
%	\Rightarrow\,& a_1-a_0 = 0\\
%	\Rightarrow\,& a_1=a_0\\
%\end{aligned}
%\end{equation*}
%
%This uniqueness makes means that collision of two strings $s_0$ and $s_1$ is only possible if for all $j$:
%\[f(s_0)_j=f(s_1)_j\]
%Hence,
%if for a given set of strings $S$ we choose $f_j$ such that this equality doesn't occur $h$ will be a perfect hash on $S$.

\subsection{Collision}
We want to know when two strings $s_0$ and $s_1$ collide.
Assume at least one of $b_j$ is nonzero,
else $h(s) = 0$ for all $s$.
Then, 
without loss of generality,
Assume that $b_n\neq 0$.
Then expansion of definitions shows that $h(s_1)=h(s_0)$ if and only if:
\[f(s_1)_n = f(s_0)_n+b_n^{-1}\sum_{j=1}^{n-1}b_j(f(s_0)_j+f(s_1)_j)\]
If we assume that $f(s)_n$ has a uniform distribution,
then the chance of colliding is $|GF(2^8)|^{-1} = 2^{-8}$.

\subsection{Choice of $a_i$}
Note that the example choice of $a_i$ makes $h$ as perfect minimal hash.
But we can choose the values of $a_i$ for other functions too,
for example classification.

\begin{center}
\begin{tabular}{|c|c|}
	\hline
	$s_i$ & $a_i$\\
	\hline
	Apple& 0\\
	Orange& 1\\
	Pear& 0\\
	Lemon& 1\\
	\hline
\end{tabular}
\end{center}
$h(s)$ is now a boolean classification function that indicates whether or not the fruit is citrus of not.

\subsection{Choice of $f_j$}
The example choice of $f(s)_j$,
where $f(s)_j$ is the $j$th character of $s$, has obvious shortcomings, consider the set:
\[S = \{\text{APPLE},\text{APRICOT}\}]\]
$M$ fails to be invertible because all string share the same first two characters and hence $M$ has a duplicate row. 
\[
M = \begin{bmatrix}
	01&10\\
	01&10\\
\end{bmatrix}
\]

This is an unfortunately common problem to encounter when $S$ is composed of natural language strings.
Not only do such strings contain patterns which cause an nonuniform distribution of characters,
alternating constant and vowel clusters for example,
but in many application the strings will share entire morphines at the beginning.
\\

For example, 
imagine making a hash function in a parser that is meant to convert strings representing command to a jump table index.
It is possible that multiple commands will start with "set\_" or "get\_".
\\

But duplication alone underestimates the size of the shortfall.
For $M$ to be uninvertible only requires that some subset of rows is linearly dependent,
which includes duplication,
and makes a choice of $f(s)_j$ all the more important.
\\

The easiest way to avoid this problem is by not taking character directly from the beginning but instead starting in the middle and take differently sized steps.
Consider the following set:
\[S = \{\text{set\_APPLE},\,\text{set\_APRICOT},\,\text{get\_APPLE},\,\text{get\_APRICOT}\}\]
In this case we take the $7$th and $9$th characters instead.
\\

This solution boils down to finding an $n$ dimensional array $\sigma$,
such that when $f(s)_j$ is the $\sigma(j)$th character of $s$ the resulting $M$ is invertible.
This search can be easily automated and done at whatever time $S$ is know,
which for many application is compile time.
