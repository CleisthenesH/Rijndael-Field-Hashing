% Copyright 2023 Kieran W Harvie. All rights reserved.

\chapter{Polynomial Methods}

\section{Interpolation}
Given two $n$ size arrays of numbers $y_k$ and $x_k$ their polynomial interpolation\footnote{
	This type of polynomial interpolation is more precisely called Lagrange to distinguish it from Hermite interpolation which includes setting the fitting the derivative at $x_k$.
}is a $n-1$ degree polynomial $p(X)$ such that:
\[y_k = p(x_k)\]
This process boils down to finding coefficients $a_k$ of $p$:
\[p(X) = \sum_{i=0}^{n-1}a_kX^k\]
This is already a kind of hash,
where we hash the elements $x_k$ to $y_k$.

This hash is a bit primitive,
and evaluating a polynomials in the na誰ve way would be too expensive,
but with some work they can be used as a base for better hashes.

\subsection{Lagrange Interpolation} 
The Lagrange Method is the most direct way to interpolate some points into a polynomial, and also gives a nice closed form for the polynomial.

Given $x_k$ define the Lagrange basis functions as:
\[L_j(X) = \prod_{\substack{j=0 \\k\neq j}}^{n-1}\frac{X-x_k}{x_j-x_k}\]

Here the $\neq$ under the $\prod$ means we skip the term where $k = j$.
For example consider the $n=3$ case with $\{x_0,x_1,x_2\}$:
\begin{equation*}
\begin{aligned}
L_0(X) =& \frac{X-x_1}{x_0-x_1}\frac{X-x_2}{x_0-x_2} \\
L_1(X) =& \frac{X-x_0}{x_1-x_0}\frac{X-x_2}{x_1-x_2} \\
L_2(X) =& \frac{X-x_0}{x_2-x_0}\frac{X-x_1}{x_2-x_1} \\
\end{aligned}
\end{equation*}
The utility of these functions is the values they take on $x_k$:
\begin{equation*}
\begin{aligned}
L_0(x_0) =& \frac{x_0-x_1}{x_0-x_1}\frac{x_0-x_2}{x_0-x_2} = 1\times 1 = 1\\
L_0(x_1) =& \frac{x_1-x_1}{x_0-x_1}\frac{x_1-x_2}{x_0-x_2} = 0\times\frac{x_1-x_2}{x_0-x_2} = 0\\
L_0(x_2) =& \frac{x_2-x_1}{x_0-x_1}\frac{x_2-x_2}{x_0-x_2} = \frac{x_2-x_1}{x_0-x_1}\times 0 = 0\\
\end{aligned}
\end{equation*}
(And likewise for $L_1(X)$ and $L_2(X)$).
\\

We get the interpolation by multiplying these functions by $y_k$ and summing:
\[p(X) = \sum_{k=0}^{n-1}y_kL_k(X)\]
This can be verified by evaluation:
\begin{equation*}
\begin{aligned}
p(x_0) =& y_0L(x_0) +y_1L(x_0)+y_2L(x_0) \\
=& y_0\times 1 + y_1\times 0 + y_2\times 0 \\
=& y_0\\
p(x_1) =& y_0L(x_1) +y_1L(x_1)+y_2L(x_0) \\
=& y_0\times 0 + y_1\times 1 + y_2\times 0 \\
=& y_1\\
p(x_2) =& y_0L(x_2) +y_1L(x_2)+y_2L(x_2) \\
=& y_0\times 0 + y_1\times 0 + y_2\times 1 \\
=& y_2\\
\end{aligned}
\end{equation*}

This method is useful for lower values of $n$ but expanding the Lagrange basis functions becomes costly as $n$ increase.
So a better method is needed.
\\

Before showing the better method there's one cool thing the Lagrange Method does, 
it enables us to reuse previous work.
For example say we have an interpolation $p$ for $x_k$, $y_k$, and $n$ but we want to add another element $y$ and $x$
Then we can get the updated interpolation $P(X)$ by:
\[P(X) = p(X)+(y-p(X))\prod_{k=0}^{n-1}\frac{X-x_k}{x-x_k}\]
When $X=x_k$ the $\prod$ term is $0$ and when $X=x$ it's $1$ meaning the two $p(X)$s cancel,
which I think is neat.

\subsection{Vandermonde Matrix}
\hyperref[matrix:useful-preamble]{We have previously seen} that field operations are compatible with matrices.
In particular that we have compute the inverse of non-singular matrices,
so if we can express interpolation as a linear matrix equation we can reuse and efficient algorithm.
\\

To this end define the Vandermonde Matrix\footnote{
	Some authors define the Vandermonde Matrix as having the highest powers on the left or $x_0$ on the bottom.
	Such minor change don't change the overall structure of the application. }
$V$ as:
\[V(x_0,x_1,\dots,x_n) = \begin{bmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{bmatrix}\]

It's utility comes from the fact we can write:
\[
\begin{bmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{bmatrix}
\begin{bmatrix}
	a_0\\a_1\\a_2\\\vdots\\a_n\\
\end{bmatrix}
=
\begin{bmatrix}
	y_0\\y_1\\y_2\\\vdots\\y_n\\
\end{bmatrix}
\]
Which is precisely the type of matrix equation that Gaussian Elimination can easily invert.
\\

The only loose end is under what conditions is the Vandermonde Matrix invertible.
The closed from of Lagrange Basis Functions is useful here,
observe that the functions are all well defined provided $x_m \neq x_n$ and once we have the functions interpolation directly follows.
Since a $n-1$ degree polynomial is uniquely defined by its value at $n$ points because Vandermonde and Lagrange are calculating the same polynomial we would also expect the matrix to be invertible when $x_m \neq x_n$.

This intuition can be made rigours by calculating the determinate of the Vandermonde Matrix.
\hyperref[appx:vandermonde-polynomial]{The full calculation can be found in the appendix,}
but cutting to the chase we have:
\[\det(V(x_0,x_1,\dots,x_n)) = \prod_{0\leq i \leq j \leq n-1}(x_j-x_i)\]
This result is called the Vandermonde Polynomial and has wider application that just inverting the Vandermonde Matrix but for our purposes it matches our intuition.
The determinate is non zero when $x_n\neq x_m$ and zero when $x_n=x_m$, for some $n$ and $m$.

\section{Evaluation}
An important application for hashing is improving performance,
hence there efficient polynomial evaluation is a must.
For historical reasons algorithms that evaluate polynomials are called schemes and in this section I will be convening three schemes.

\subsection{Na誰ve Scheme}
Let $a_k$ be the $k^\text{th}$ order coefficient for an $n$ degree polynomial.
Then the na誰ve method of evaluating the polynomial at $x$ involves evaluating the first $n$ powers of $x$.
Multiplying the powers by the coefficients then summing them together.
\\
For example, consider evaluating:
\[2+x+3x^2+x^3\]
at $x=2$.
\begin{enumerate}
	\item Calculate a list of powers: $x^0 = 1,\,x^1 = 2,\,x^2 = 4,\,x^3 = 8$.
	\item Multiply by coefficients: $a_0x_0 = 2,\,a_1x^1 = 2,\,a_2x^2 = 12,\, a_3x^3 = 8$.
	\item Sum all the results: $2+2+12+8 = 24$.
\end{enumerate}
Assuming we evaluated the powers of $x$ by multiplying the previous power by $x$ instead of starting from scratch,
this method takes $n-1$ multiplications to evaluate the powers,
$n$ multiplications for multiplying the coefficients by the powers,
and $n-1$ additions.

\subsection{Horner's Scheme}
Horner's scheme is an iterative polynomial evaluation scheme where we combine the multiplication for generating powers with the multiplication for the coefficients.
Algebraically, it looks like this:
\[a_0+a_1x+a_2x^2+a_3x^3 = a_0+x(a_1+x(a_2+x(a_3)))\]
Algorithmically, we initialize with: 
\[y_0 = a_n\]
Then iterate with:
\[y_k = a_{n-k}+xy_{k-1}\]
\\
There are $n$ iterative steps and each step involves one multiplication and one addition.
Reusing the na誰ve example we get:
\begin{enumerate}
	\item Initialize: $y_0 = 1$
	\item Iterate: $y_1 = a_2+xy_1 = 3+2\times1 = 5$
	\item Iterate: $y_2 = a_1+xy_2 = 1+2\times5 = 11$
	\item Iterate: $y_3 = a_0+xy_3 = 2+2\times11 = 24$
\end{enumerate}
Which gives the same result of $24$ but for only three multiplications and three additions.

\subsection{Estrin's Scheme}
In terms of number of multiplications, Horner's Scheme is the best and one of the most common evaluation scheme for general polynomial evaluation.
But there is one flaw.
Every iterative step requires the previous to be completed,
which really limits the ability to parallelize the algorithm.
\\

Estrin's scheme converts an $n$ degree evaluation at $x$ into a $\lfloor n/2 \rfloor$ degree evaluation at $x^2$ using $\lceil n/2 \rceil$ additions and multiplications that can be done in parallel,
algebraically:
\[a_0+a_1x+a_2x^2+a_3x^3 = (a_0+xa_1)+x^2(a_2+xa_3)\]
Algorithmically,
we input an $n$-array $a_k$ of coefficients along side $x$ and output the $\lceil n/2 \rceil$-array $b_k$ along side $x^2$ such that:
\[b_k = a_{2k}+xa_{2k+1}\]
And iterate until we have a $1$-array with the value as it's only element.

\begin{enumerate}
	\item Input: $[2,1,3,1]$ and $x=2$
	\item Iterate: $[2+1\times2,3+1\times2] = [4,5]$ and $x=4$
	\item Iterate: $[4+5\times4] = [24]$ and $x=8$
\end{enumerate}

\section{Application to Hashing}
Minimal perfect hashing.
