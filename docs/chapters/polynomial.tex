% Copyright 2023 Kieran W Harvie. All rights reserved.

\chapter{Polynomial Methods}

\section{Interpolation}
Given two $n$ size arrays of numbers $y_k$ and $x_k$ their polynomial interpolation\footnote{
	This type of polynomial interpolation is more precisely called Lagrange to distinguish it from Hermite interpolation which includes setting the fitting the derivative at $x_k$.
}is a $n-1$ degree polynomial $p(X)$ such that:
\[y_k = p(x_k)\]
This process boils down to finding coefficients $a_k$ of $p$:
\[p(X) = \sum_{i=0}^{n-1}a_kX^k\]
This is already a kind of hash,
where we hash the elements $x_k$ to $y_k$.

This hash is a bit primitive,
and evaluating a polynomials in the na√Øve way would be too expensive,
but with some work they can be used as a base for better hashes.

\subsection{Lagrange Interpolation} 
The Lagrange Method is the most direct way to interpolate some points into a polynomial, and also gives a nice closed form for the polynomial.

Given $x_k$ define the Lagrange basis functions as:
\[L_j(X) = \prod_{\substack{j=0 \\k\neq j}}^{n-1}\frac{X-x_k}{x_j-x_k}\]

Here the $\neq$ under the $\prod$ means we skip the term where $k = j$.
For example consider the $n=3$ case with $\{x_0,x_1,x_2\}$:
\begin{equation*}
\begin{aligned}
L_0(X) =& \frac{X-x_1}{x_0-x_1}\frac{X-x_2}{x_0-x_2} \\
L_1(X) =& \frac{X-x_0}{x_1-x_0}\frac{X-x_2}{x_1-x_2} \\
L_2(X) =& \frac{X-x_0}{x_2-x_0}\frac{X-x_1}{x_2-x_1} \\
\end{aligned}
\end{equation*}
The utility of these functions is the values they take on $x_k$:
\begin{equation*}
\begin{aligned}
L_0(x_0) =& \frac{x_0-x_1}{x_0-x_1}\frac{x_0-x_2}{x_0-x_2} = 1\times 1 = 1\\
L_0(x_1) =& \frac{x_1-x_1}{x_0-x_1}\frac{x_1-x_2}{x_0-x_2} = 0\times\frac{x_1-x_2}{x_0-x_2} = 0\\
L_0(x_2) =& \frac{x_2-x_1}{x_0-x_1}\frac{x_2-x_2}{x_0-x_2} = \frac{x_2-x_1}{x_0-x_1}\times 0 = 0\\
\end{aligned}
\end{equation*}
(And likewise for $L_1(X)$ and $L_2(X)$).
\\

We get the interpolation by multiplying these functions by $y_k$ and summing:
\[p(X) = \sum_{k=0}^{n-1}y_kL_k(X)\]
This can be verified by evaluation:
\begin{equation*}
\begin{aligned}
p(x_0) =& y_0L(x_0) +y_1L(x_0)+y_2L(x_0) \\
=& y_0\times 1 + y_1\times 0 + y_2\times 0 \\
=& y_0\\
p(x_1) =& y_0L(x_1) +y_1L(x_1)+y_2L(x_0) \\
=& y_0\times 0 + y_1\times 1 + y_2\times 0 \\
=& y_1\\
p(x_2) =& y_0L(x_2) +y_1L(x_2)+y_2L(x_2) \\
=& y_0\times 0 + y_1\times 0 + y_2\times 1 \\
=& y_2\\
\end{aligned}
\end{equation*}

This method is useful for lower values of $n$ but expanding the Lagrange basis functions becomes costly as $n$ increase.
So a better method is needed.
\\

Before showing the better method there's one cool thing the Lagrange Method does, 
it enables us to reuse previous work.
For example say we have an interpolation $p$ for $x_k$, $y_k$, and $n$ but we want to add another element $y$ and $x$
Then we can get the updated interpolation $P(X)$ by:
\[P(X) = p(X)+(y-p(X))\prod_{k=0}^{n-1}\frac{X-x_k}{x-x_k}\]
When $X=x_k$ the $\prod$ term is $0$ and when $X=x$ it's $1$ meaning the two $p(X)$s cancel,
which I think is neat.

\subsection{Vandermonde Matrix}
\hyperref[matrix:useful-preamble]{We have previously seen} that field operations are compatible with matrices.
In particular that we have compute the inverse of non-singular matrices,
so if we can express interpolation as a linear matrix equation we can reuse and efficient algorithm.
\\

To this end define the Vandermonde Matrix\footnote{
	Some authors define the Vandermonde Matrix as having the highest powers on the left or $x_0$ on the bottom.
	Such minor change don't change the overall structure of the application. }
$V$ as:
\[V(x_0,x_1,\dots,x_n) = \begin{bmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{bmatrix}\]

It's utility comes from the fact we can write:
\[
\begin{bmatrix} 
	1&x_0&x_0^2&\dots&x_0^n \\
	1&x_1&x_1^2&\dots&x_1^n \\
	1&x_2&x_2^2&\dots&x_2^n \\
	\vdots&\vdots&\vdots&\ddots&\vdots \\
	1&x_n&x_n^2&\dots&x_n^n \\
\end{bmatrix}
\begin{bmatrix}
	a_0\\a_1\\a_2\\\vdots\\a_n\\
\end{bmatrix}
=
\begin{bmatrix}
	y_0\\y_1\\y_2\\\vdots\\y_n\\
\end{bmatrix}
\]
Which is precisely the type of matrix equation that Gaussian Elimination can easily invert.
\\

The only loose end is under what conditions is the Vandermonde Matrix invertible.
The closed from of Lagrange Basis Functions is useful here,
observe that the functions are all well defined provided $x_m \neq x_n$ and once we have the functions interpolation directly follows.
Since a $n-1$ degree polynomial is uniquely defined by its value at $n$ points because Vandermonde and Lagrange are calculating the same polynomial we would also expect the matrix to be invertible when $x_m \neq x_n$.

This intuition can be made rigours by calculating the determinate of the Vandermonde Matrix.
\hyperref[appx:laplace-expansion]{The full calculation can be found in the appendix,}
but cutting to the chase we have:
\[\det(V(x_0,x_1,\dots,x_n)) = \prod_{0\leq i \leq j \leq n-1}(x_j-x_i)\]
This result is called the Vandermonde Polynomial and has wider application that just inverting the Vandermonde Matrix but for our purposes it matches our intuition.
The determinate is non zero when $x_n\neq x_m$ and zero when $x_n=x_m$, for some $n$ and $m$.
\section{Evaluation}
\subsection{Horner's scheme}
\[y_0 = a_n\]
\[y_k = a_{n-k}+xy_{k-1}\]
\subsection{Estrin's scheme}
\subsection{Parallel evaluation}
\section{Application to Hashing}
Minimal perfect hashing.
